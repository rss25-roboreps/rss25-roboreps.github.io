<!DOCTYPE html>
<html lang="en">
<!-- css from https://github.com/ai-workshops/ai-workshops.github.io/blob/master/generalizable-policy-learning-in-the-physical-world/style.css -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
  integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

<link rel="icon"
  href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text x=%22-.1em%22 y=%22.9em%22 font-size=%2280%22>ü§ñ</text></svg>">

<!-- jQuery library -->
<script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<!-- Latest compiled JavaScript -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js"
  integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>



<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
  <title>Workshop on Learned Robot Representations | RSS 2025</title>
  <link rel="stylesheet" href="css/style.css">

  <style>
    .collapsible {
      background-color: #fff;
      color: #000;
      cursor: pointer;
      padding: 18px;
      width: 100%;
      border: 0px solid white;
      text-align: center;
      outline: none;
      font-size: 15px;
    }

    .title-container {
      width: 100%;
      margin: auto;
      padding: 40px 20px;
      color: black;
      background-image: linear-gradient(135deg, #333, black, #333);
      background: white;
      /* background-image: url("imgs/corl2024_background.png"); */
      background-size: cover;
      background-position: center;
      background-repeat: no-repeat, repeat;
      display: flex;
      flex-direction: column;
      justify-content: center;
      margin-top: 55px;
    }

    .active,
    .collapsible:hover {
      background-color: #bbb;
    }

    .content {
      padding: 5px 18px;
      display: none;
      overflow: hidden;
      background-color: #f1f1f1;
    }

    .triangle-up {
      width: 0;
      height: 0;
      border-left: 10px solid transparent;
      border-right: 10px solid transparent;
      border-bottom: 20px solid rgb(255, 255, 255);
      float: right;
    }

    .triangle-down {
      width: 0;
      height: 0;
      border-left: 10px solid transparent;
      border-right: 10px solid transparent;
      border-top: 20px solid rgb(255, 255, 255);
      float: right;
    }
  </style>




</head>

<body>
  <nav class="navbar navbar-expand-xl navbar-expand-lg navbar-expand-custom navbar-fixed-top sticky-nav">
    <button class="navbar-toggler navbar-light" type="button" data-toggle="collapse" data-target="#main-navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="main-navigation">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="index.html#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#call">Call for Papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#dates">Dates</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#schedule">Schedule</a>
        </li>
        <!-- <li class="nav-item">
            <a class="nav-link" href="index.html#program">Program</a>
          </li> -->
        <!-- <li class="nav-item">
          <a class="nav-link" href="index.html#related">Related Workshops</a>
        </li> -->
        <li class="nav-item">
          <a class="nav-link" href="index.html#committee">Committees</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="title-container">
    <div style="text-align: center;">
      <!-- <a href="https://www.corl.org/" target="_blank"><img src="imgs/corl2024.png" style="width:250px;"></a> -->
      <br>
      <div class="subtitle">Workshop on </div>
      <h1 style="width:80%;margin:auto;">Learned Robot Representations (RoboReps)</h1>
      <div class="subtitle" style="color: #130b69; margin: 20px; margin-bottom: -10px;">
        RSS 2025 | Los Angeles, CA | Wednesday, June 25th, 2025 | Room: <a
          href="https://www.google.com/maps/place/Seeley+G.+Mudd+Building(SGM)/@34.0213147,-118.2916414,838m/data=!3m2!1e3!4b1!4m6!3m5!1s0x80c2c7fb5d77440d:0xcc7befd3b57858ae!8m2!3d34.0213103!4d-118.2890665!16s%2Fg%2F11bwyltwpl?entry=tts&g_ep=EgoyMDI1MDYwNC4wIPu8ASoASAFQAw%3D%3D&skid=8e36b5de-be6c-4493-97db-30257223647e">SGM
          124</a>
      </div>
      <br>
      <h3>Email: <a href="mailto:rss25.roboreps@gmail.com">rss25.roboreps@gmail.com</a></h3>
    </div>
  </div>

  <div class="container" style="padding-bottom: 0px;">
    <div class="section" id="overview">
      <h2>Overview</h2>
      <p>General-purpose robotic systems require powerful representations and abstractions. In deployment, such robots
        are expected to encounter diverse and complex scenarios. While recent large-scale learned models exhibit
        remarkable generalization, similarly getting representations that can flexibly generalize to all the
        unanticipated situations a robot might face remains challenging, especially given the cost of robot data. Thus,
        it is important to investigate how to best learn generalizable representations, evaluate their effectiveness,
        and leverage them for downstream robotics tasks.</p>
      <p>Ideally, these representations should capture: (1) spatial-dynamic information needed for fine-grained control,
        (2) semantic information required for common-sense reasoning and scene understanding, and (3) knowledge of
        conventions needed for smooth human-robot interactions. Additionally, these representations must be robust to
        the diversity of tasks, scenes, and operators the robot will encounter. In this workshop, we aim to explore the
        following: What makes a good robot representation, how can we learn them, and how can we most effectively make
        use of them?</p>
      <p>Our speakers and panelists are pioneering robotics and machine learning researchers defining the state of the
        art on a range of topics, including: end-to-end control, task and motion planning (TAMP), human-robot
        interaction (HRI), scene understanding / SLAM, and more. We invite the community for submissions in these areas
        as well as from a wider set of perspectives ‚Äì for example, submissions addressing how the following fields might
        guide robotics research: (1) deep representation learning in vision and language; (2) learning representations
        for field robotics and AI, where data is extremely scarce or noisy; or (3) bias and robustness in neural
        representations.</p>


    </div>

    <div class="section" id="objectives">
      <!-- What sorts of pre-training tasks, objectives, data, or models yield good representations for robotics?
      How do we evaluate or choose good pre-trained models to fine-tune into robot policies or leverage in a perception or control stack?
      Is ‚Äúembodied‚Äù or ‚Äúgrounded‚Äù data necessary for good robot representations, or is general pre-training data sufficient?
      When is fine-tuning large models from other domains enough for robotic foundation models and when do we need robot-specific large-scale training?
      How do we elicit representations at a level of granularity and semantic abstraction that is appropriate for robot tasks?
      What functionalities do pre-trained representations grant robots? E.g.:
      Common-sense semantic understanding (‚Äúwhat‚Äù to look for)
      Reasoning about semantics, dynamics, spatial features, or motion (‚Äúhow‚Äù to act)
      Conventions for interacting with humans (dialogue, goal specification, information gathering, interpretability)
      How are these pre-trained representations best integrated into robot systems? E.g.,
      End-to-end learned policies learned with BC or RL (e.g., RT-2, OpenVLA)
      Zero-shot controllers (e.g., Code as Policies, MOKA)
      Hand-crafted scene or task representations (e.g., semantic scene graphs, LeRFs, neural TAMP, image generation as subgoals)
      How can learned representations be de-biased such that the robots using them can:
      Robustly operate in underrepresented environments (e.g., households from cultures around the world, not just ‚Äúconventional‚Äù households)
      Understand language and behaviors from diverse human operators -->



      <h3 style="text-align: center;">Areas of Interest</h3>
      <p>We aim to investigate the following topics and research questions:
      <ul>
        <li> What sorts of pre-training tasks, objectives, data, or models yield good representations for robotics? <ul>
            <li>How do we evaluate or choose good pre-trained models to fine-tune into robot policies or leverage in a
              perception or control stack?</li>
            <li>Is ‚Äúembodied‚Äù or ‚Äúgrounded‚Äù data necessary for good robot representations, or is general pre-training
              data sufficient?</li>
            <li>When is fine-tuning large models from other domains enough for robotic foundation models and when do we
              need robot-specific large-scale training?</li>
            <li>How do we elicit representations at a level of granularity and semantic abstraction that is appropriate
              for robot tasks?</li>
          </ul>
        <li> What functionalities do pre-trained representations grant robots? E.g.: <ul>
            <li>Common-sense semantic understanding (‚Äúwhat‚Äù to look for)</li>
            <li>Reasoning about semantics, dynamics, spatial features, or motion (‚Äúhow‚Äù to act)</li>
            <li>Conventions for interacting with humans (dialogue, goal specification, information gathering,
              interpretability)</li>
          </ul>
        <li> How are these pre-trained representations best integrated into robot systems? E.g.: <ul>
            <li>End-to-end learned policies learned with BC or RL (e.g., RT-2, OpenVLA)</li>
            <li>Zero-shot controllers (e.g., Code as Policies, MOKA)</li>
            <li>Hand-crafted scene or task representations (e.g., semantic scene graphs, LeRFs, neural TAMP, image
              generation as subgoals)</li>
          </ul>
        <li> How can learned representations be de-biased such that the robots using them can:<ul>
            <li>Robustly operate in underrepresented environments (e.g., households from cultures around the world, not
              just ‚Äúconventional‚Äù households)</li>
            <li>Understand language and behaviors from diverse human operators</li>
          </ul>
      </ul>
      </p>
      <p>We also give a non-exhaustive list of keywords:
      <ul>
        <li>Foundation models for (zero-shot) planning, control, reasoning, scene understanding/SLAM, and HRI</li>
        <li>Representation learning for policy learning: vision-language-action models (VLAs), reinforcement learning,
          cross-embodiment transfer</li>
        <li>Representation learning for perception and scene understanding: NeRF/LeRF, Gaussian splatting, inverse
          graphics/reconstruction, metric-semantic scene graphs</li>
        <li>Representation learning for planning: neural TAMP, learned dynamics/world modeling, chain-of-thought
          reasoning, generated image subgoals</li>
        <li>Data for embodied, grounded, or spatial reasoning</li>
      </ul>
      </p>
    </div>

    <div class="section" id="call">

      <h2>Submission Guidelines</h2>

      <h3>Submission Portal (NOW CLOSED): <a
          href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2025/Workshop/RoboReps"
          target="_blank"><strike>OpenReview</strike></a></h3>

      <p>
        We are accepting workshop submissions of the following types
      <ul>
        <li> Papers - up to 6 pages plus unlimited references / appendices</li>
        <li> Extended Abstracts - up to 2 pages plus unlimited references / appendices</li>
      </ul>
      </p>

      <p>
        We request that submissions are in the RSS format. They should <i>not</i> be
        anonymized. Additionally, you <i>may</i> submit papers that are under review at other venues or submitted to
        other workshops.
      </p>

      <br>
      <!-- <h3 style="text-align: center;"><b>Accepted papers can be found on <a
            href="https://openreview.net/group?id=robot-learning.org/CoRL/2023/Workshop/LEAP">OpenReview</a>.</b></h3> -->

      <!-- <h2>Best Paper: <a href="https://openreview.net/forum?id=ZGbWq3VqrO" target="_blank">ReKep: Spatio-Temporal
          Reasoning of Relational Keypoint Constraints for Robotic Manipulation</a></h2>

      <center><img src="imgs/LEAP24_best_paper.png" width="500"></center> -->

    </div>



    <div class="section" id="dates" style="align-content: center;">
      <h2>Important Dates</h2>
      <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
        <tr>
          <td class="noborder">Paper Submission Deadline
          </td>
          <td class="noborder"><strike><b class="font-weight-bold">May 28, 2025 - 23:59 AOE</b></strike>
          </td>
        </tr>
        <tr>
          <td class="noborder">Paper Acceptance
          </td>
          <td class="noborder"><strike><b class="font-weight-bold">June 11, 2025</b></strike>
          </td>
        </tr>
        <tr>
          <td class="noborder">Camera-ready Version Due
          </td>
          <td class="noborder"><b class="font-weight-bold">June 16, 2025 - 23:59 AOE</b>
          </td>
        </tr>
        <tr>
          <td class="noborder">Workshop
          </td>
          <td class="noborder"><b class="font-weight-bold">June 25, 2025</b>
          </td>
        </tr>
      </table>
    </div>

    <div class="section" id="schedule" style="align-content: center;">
      <h2>Schedule</h2>

      <tr>
        <td> </td>
      </tr>

      <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
        <tr>
          <td colspan="2">
            <h4>Session 1</h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"> 8:40 AM - 8:50 AM</td>
          <td class="noborder"> <b>Opening Remarks</b></td>
        </tr>
        <tr>
          <td class="noborder"> 8:50 AM - 9:30 AM</td>
          <td class="noborder"> <b>Invited Talk 1: Wolfram Burgard (Virtual)</b></td>
        </tr>
        <tr>
          <td class="noborder"> 9:30 AM - 10:20 AM</td>
          <td class="noborder"> <b>Poster Session A, Coffee Break</b></td>
        </tr>
        <tr>
          <td colspan="2">
            <h4>Session 2</h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"> 10:20 AM - 11:55 AM</td>
          <td class="noborder"> <b>Invited Talks 2, 3, 4: Liam Paull, Chelsea Finn, Mahi Shafiullah</b></td>
        </tr>
        <tr>
          <td class="noborder"> 11:55 AM - 12:30 PM</td>
          <td class="noborder"> <b>Panel</b></td>
        </tr>
        <tr>
          <td class="noborder"> 12:30 PM - 2:00 PM</td>
          <td class="noborder"> <b>Lunch Break</b></td>
        </tr>
        <tr>
          <td colspan="2">
            <h4>Session 3</h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"> 2:00 PM - 2:20 PM</td>
          <td class="noborder"> <b>Spotlight Talks</b></td>
        </tr>
        <tr>
          <td class="noborder"> 2:20 PM - 3:00 PM</td>
          <td class="noborder"> <b>Invited Talk 5: Krishna Murthy</b></td>
        </tr>
        <tr>
          <td class="noborder"> 3:00 PM - 4:00 PM</td>
          <td class="noborder"> <b>Poster Session B, Coffee Break</b></td>
        </tr>
        <tr>
          <td colspan="2">
            <h4>Session 4</h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"> 4:00 PM - 4:40 PM</td>
          <td class="noborder"> <b>Invited Talk 6: Andreea Bobu</b></td>
        </tr>
        <tr>
          <td class="noborder"> 4:40 PM - 5:00 PM</td>
          <td class="noborder"> <b>Closing Remarks</b></td>
        </tr>
      </table>



    </div>

    <div class="section" id="papers" style="align-content: center;">
      <h2>Papers and Poster Session Assignments</h2>

      <p>We shall link to all the papers very soon!</p>
      <p>Asterisk after paper title indicates spotlight talk.</p>

      <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
        <tr>
          <td colspan="1">
            <h4><b>Poster Session A: 9:30 AM - 10:20 AM</b></h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"><b>1. TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement
              Learning</b><br /><small>Ge Li, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, Gerhard
              Neumann</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>2. Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied
              Question
              Answering</b><br /><small>Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Marek Reinke, Jai
              Krishna Bandi, Navid Kayhani, Oriana Peltzer, David Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel
              Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>3. Learning Attentive Neural Processes for Planning with Pushing
              Actions</b><br /><small>Atharv Jain, Seiji A Shaw, Nicholas Roy</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>4. Interpretable Human-in-the-Loop In-Context Preference Learning Via Preference
              Boundaries</b><br /><small>Valerie K. Chen, Julie Shah, Andreea Bobu</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>5. Online Latent Factor Representation Learning</b><br /><small>Alejandro
              Murillo-Gonz√°lez, Lantao Liu</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>6. DexWild: Dexterous Human Interactions for In-the-Wild Robot
              Policies*</b><br /><small>Tony Tao, Mohan Kumar Srirama, Jason Jingzhou Liu, Kenneth Shaw, Deepak
              Pathak</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>7. GRIM: Task-Oriented Grasping with Conditioning on Generative
              Examples</b><br /><small>Shailesh, Alok Raj, Nayan Kumar, Priya Shukla, Andrew Melnik, Michael Beetz, Gora
              Chand Nandi</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>8. Bi-Manual Joint Camera Calibration and Scene Representation</b><br /><small>Haozhan
              Tang, Tianyi Zhang, Matthew Johnson-Roberson, William Zhi</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>9. DisDP: Robust Imitation Learning via Disentangled Diffusion
              Policies</b><br /><small>Pankhuri Vanjani, Paul Mattes, Kevin Daniel Kuryshev, Xiaogang Jia, Vedant Dave,
              Rudolf Lioutikov</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>10. RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and
              Exploration</b><br /><small>Omar Alama, Avigyan Bhattacharya, Haoyang He, Seungchan Kim, Yuheng Qiu,
              Wenshan Wang, Cherie Ho, Nikhil Varma Keetha, Sebastian Scherer</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>11. Learning Symbolic World Model Representations for Long-Horizon Robot
              Planning</b><br /><small>Naman Shah, Jayesh Nagpal, Siddharth Srivastava</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>12. WoMAP: World Models For Embodied Open-Vocabulary Object
              Localization*</b><br /><small>Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Ola Sho, Emily Zhou, Miyu
              Yamane,
              Jeremy Bao, Anirudha Majumdar</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>13. ReWiND: Language-Guided Rewards Teach Robot Policies without New
              Demonstrations*</b><br /><small>Jiahui Zhang, Yusen Luo, Abrar Anwar, Sumedh Anand Sontakke, Joseph J Lim,
              Jesse Thomason, Erdem Biyik, Jesse Zhang</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>14. Importance Weighted Retrieval for Few-Shot Imitation
              Learning</b><br /><small>Amber
              Xie, Rahul Chand, Dorsa Sadigh, Joey Hejna</small></td>
        </tr>
      </table>

      <br />

      <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
        <tr>
          <td colspan="1">
            <h4><b>Poster Session B: 3:00 PM - 4:00 PM</b></h4>
          </td>
        </tr>
        <tr>
          <td class="noborder"><b>1. Learning Factorized Diffusion Policies for Conditional Action
              Diffusion</b><br /><small>Omkar Patil, Prabin Kumar Rath, Kartikay Milind Pangaonkar, Eric Rosen, Nakul
              Gopalan</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>2. DREAM: Differentiable Real-to-Sim-to-Real Engine for Learning Robotic
              Manipulation</b><br /><small>Haozhe Lou, Mingtong Zhang, Haoran Geng, Hanyang Zhou, Sicheng He, Zhiyuan
              Gao, Siheng Zhao, Jiageng Mao, Pieter Abbeel, Jitendra Malik, Daniel Seita, Yue Wang</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>3. Learning Long-Context Diffusion Policies via Past-Token
              Prediction*</b><br /><small>Marcel Torne, Andy Tang, Yuejiang Liu, Chelsea Finn</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>4. H<sup>3</sup>DP: Triply‚ÄëHierarchical Diffusion Policy for Visuomotor
              Learning</b><br /><small>Yiyang Lu, Yufeng Tian, Zhecheng Yuan, Xianbang Wang, Pu Hua, Zhengrong Xue,
              Huazhe Xu</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>5. Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation
              Datasets</b><br /><small>Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Pannag R Sanketi, Ken Goldberg</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>6. Implicit Contact Representations with Neural Descriptor Fields for Learning Dynamic
              Recovery Policies</b><br /><small>Fan Yang, Sergio Francisco Aguilera Marinovic, Soshi Iba, Rana Soltani
              Zarrin, Dmitry Berenson</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>7. CL-HCoTNav: Closed-Loop Hierarchical Chain-of-Thought for Zero-Shot Object-Goal
              Navigation with Vision-Language Models</b><br /><small>Yuxin Cai, Haoruo Zhang, Wei-Yun Yau, Chen
              Lv</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>8. XPG-RL: Reinforcement Learning with Explainable Priority Guidance for
              Efficiency-Boosted Mechanical Search</b><br /><small>Yiting Zhang, Shichen Li, Elena Shrestha</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>9. Point Policy: Unifying Observations and Actions with Key Points for Robot
              Manipulation</b><br /><small>Siddhant Haldar, Lerrel Pinto</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>10. A Steerable Vision-Language-Action Framework for Autonomous
              Driving</b><br /><small>Tian Gao, Catherine Glossop, Kyle Stachowicz, Timothy Gao, Celine Tan, Oier Mees,
              Yuejiang Liu, Sergey Levine, Dorsa Sadigh, Chelsea Finn</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>11. GraphSeg: Segmented 3D Representations via Graph Edge Addition and
              Contraction</b><br /><small>Haozhan Tang, Tianyi Zhang, Matthew Johnson-Roberson, William Zhi</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>12. Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy
              Learning*</b><br /><small>Sunghwan Kim, Woojeh Chung, Yulun Tian, Zhirui Dai, Arth Shukla, Hao Su, Nikolay
              Atanasov</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>13. SkillWrapper: Autonomously Learning Interpretable Skill Abstractions with
              Foundation
              Models</b><br /><small>Ziyi Yang, Benned Hedegaard, Ahmed Jaafar, Skye Thompson, Yichen Wei, Everest Yang,
              Haotian Fu, Shreyas Sundara Raman, Stefanie Tellex, George Konidaris, David Paulius, Naman Shah</small>
          </td>
        </tr>
        <tr>
          <td class="noborder"><b>14. Structured 3D Scene Queries with Graph Databases</b><br /><small>Aaron Ray, Luca
              Carlone</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>15. EgoZero: Robot Learning from Smart Glasses*</b><br /><small>Vincent Liu, Ademi
              Adeniji,
              Haotian Zhan, Raunaq Bhirangi, Pieter Abbeel, Lerrel Pinto</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>16. Feel the Force: Contact-Driven Learning from Humans</b><br /><small>Ademi Adeniji,
              Zhuoran Chen, Vincent Liu, Venkatesh Pattabiraman, Siddhant Haldar, Raunaq Bhirangi, Pieter Abbeel, Lerrel
              Pinto</small></td>
        </tr>
        <tr>
          <td class="noborder"><b>17. BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation
              Learning</b><br /><small>Hongyi Zhou, Weiran Liao, Xi Huang, Yucheng Tang, Fabian Otto, Xiaogang Jia,
              Xinkai Jiang, Simon Hilber, Ge Li, Qian Wang, √ñmer Erdin√ß Yaƒümurlu, Nils Blank, Moritz Reuss, Rudolf
              Lioutikov</small></td>
        </tr>
      </table>



    </div>


    <!-- <div class = "secion" id="speakers" style="text-align: center; padding: 10px;">
    <h2> Inivted Speakers</h2>

  </div> -->

    <div class="section" id="speakers" style="text-align: center; padding: 10px;">
      <h2>Invited Speakers</h2>
      <div class="grid">
        <center>
          <div class="row">

            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://mahis.life/" target="_blank"><img src="imgs/Mahi_Shafiullah.jpg" class="rounded-circle"
                  alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="https://mahis.life/" target="_blank">Mahi Shafiullah</a>
              </h5>
              New York University </br> USA
            </div>
            <br /> <br />

            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://www.mit.edu/~abobu/" target="_blank"><img src="imgs/Andreea_Bobu.jpg"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="https://www.mit.edu/~abobu/" target="_blank">Andreea Bobu</a>
              </h5>
              Massachusetts Institute of Technology </br> USA
            </div>
            <br /> <br />
            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://ai.stanford.edu/~cbfinn/" target="_blank"><img src="imgs/Chelsea_Finn.jpg"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="https://ai.stanford.edu/~cbfinn/" target="_blank">Chelsea Finn</a>
              </h5>
              Stanford University & Physical Intelligence </br> USA
            </div>
            <br /> <br />

          </div>
          <div class="row">
            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="http://www2.informatik.uni-freiburg.de/~burgard/" target="_blank"><img
                  src="imgs/Wolfram_Burgard.jpg" class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="http://www2.informatik.uni-freiburg.de/~burgard/"
                  target="_blank">Wolfram Burgard</a>
              </h5>
              University of Technology Nuremberg </br> Germany
            </div>
            <br /> <br />

            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://liampaull.ca/" target="_blank"><img src="imgs/Liam_Paull.png" class="rounded-circle"
                  alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="https://liampaull.ca/" target="_blank">Liam Paull</a>
              </h5>
              University of Montreal </br> Canada
            </div>
            <br /> <br />

            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://krrish94.github.io/" target="_blank"><img src="imgs/Krishna_Murthy.png"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"><a href="https://krrish94.github.io/" target="_blank">Krishna Murthy
                  Jatavallabhula</a>
              </h5>
              META </br> USA
            </div>
            <br /> <br />

          </div>
        </center>
      </div>

    </div>


    <div class="section" id="committee" style="text-align: center; padding: 10px;">
      <h2>Organizing Committees</h2>
      <!-- <h3>Organizing Committee</h3> -->
      <br />

      <div class="grid">
        <center>
          <div class="row">
            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://verityw.github.io/" target="_blank"><img src="imgs/Will_Chen.JPG" class="rounded-circle"
                  alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://verityw.github.io/" target="_blank">William Chen</a></h5>
              U.C. Berkeley
            </div>
            <br /> <br />
            <div class="col-lg-4 col-md-3 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://dominic101.github.io/DominicMaggio/" target="_blank"><img src="imgs/Dominic_Maggio.jpg"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://dominic101.github.io/DominicMaggio/"
                  target="_blank">Dominic Maggio</a></h5>
              Massachusetts Institute of Technology
            </div>
            <br /> <br />
            <div class="col-lg-4 col-md-6 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://mlevy2525.github.io/" target="_blank"><img src="imgs/Mara_Levy.png"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://mlevy2525.github.io/" target="_blank">Mara Levy</a></h5>
              University of Maryland
            </div>
            <br /> <br />


          </div>
          <div class="row">
            <div class="col-lg-4 col-md-6 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://robodhruv.github.io/" target="_blank"><img src="imgs/Dhruv_Shah.jpeg"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://robodhruv.github.io/" target="_blank">Dhruv Shah</a></h5>
              Google DeepMind
            </div>
            <br /> <br />
            <div class="col-lg-4 col-md-6 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://jaredstrader.github.io/" target="_blank"><img src="imgs/Jared_Strader.jpg"
                  class="rounded-circle" alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://jaredstrader.github.io/" target="_blank">Jared
                  Strader</a></h5>
              Massachusetts Institute of Technology
            </div>
            <br /> <br />
            <div class="col-lg-4 col-md-6 col-sm-6 col-xs-12" style="padding-bottom: 30px;">
              <a href="https://kuanfang.github.io/" target="_blank"><img src="imgs/Kuan_Fang.jpg" class="rounded-circle"
                  alt="" width="140" height="140"></a>
              <h5 style="margin-bottom:0em;"> <a href="https://kuanfang.github.io/" target="_blank">Kuan Fang</a></h5>
              Cornell University
            </div>
            <br /> <br />

          </div>


      </div>
    </div>
    <!-- <div class="section" id="accpted" style="text-align: left; padding: 5px;">
      <h2>Accepted Papers (Read them <a target="_blank" href="">here</a>)</h2> 
      <ul>

        <li><b>Paper1</b>
          <br>Author1
        </li>
        <li><b>Paper2</b>
          <br>Author2
        </li>

      </ul>

    </div> -->

    <div class="section" id="pc" style="text-align: left; padding: 5px;">
      <h2>Program Committee</h2>
      <center>We are currently looking for program committee members!</center>


      <!-- <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">

        <tr>
          <td>TODO</td>
          <td>TODO</td>
          <td>TODO</td>
          <td>TODO</td>
        </tr>

        <tr>
          <td>TODO</td>
          <td>TODO</td>
          <td>TODO</td>
          <td>TODO</td>
        </tr>



      </table> -->


    </div>

    <div class="section" id="attribution" style="text-align: left; padding: 5px;">
      The template for this website is borrowed from the <a href="https://leap-workshop.github.io/">CoRL 2024 LEAP
        Workshop</a>. All credit goes to the creators of said website.
    </div>

    <script src="javascripts/scale.fix.js"></script>
    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;

      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function () {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
          } else {
            content.style.display = "block";
          }
        });
      }
    </script>
</body>

</html>